# -*- coding: utf-8 -*-
"""Project_Phase1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-au5mFuwU_DKIez2TOQasDtekeHMobnL

**Importing Libraries and Packages**
"""

import pandas as pd
import numpy as np
import missingno as msno
# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns
!pip install chart_studio
import chart_studio.plotly as py
import plotly.graph_objects as go
import plotly
import plotly.express as px
from plotly.offline import iplot, init_notebook_mode
import cufflinks
cufflinks.go_offline()
cufflinks.set_config_file(world_readable=True, theme='pearl')
from bokeh.palettes import viridis
colors = viridis(10)
import plotly.io as pio
pio.renderers.default = "colab"

#preprocessing 
from sklearn.preprocessing import StandardScaler, LabelEncoder
from collections import Counter
# Classification 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""Reading Data from heart.csv file into the data Frame heart_data"""

# loading the csv data to a Pandas DataFrame
heart_data = pd.read_csv('/content/heart.csv')

"""Exploring the dataset and identifying the null and missing values"""

# print first 5 rows of the dataset
heart_data.head()

# print last 5 rows of the dataset
heart_data.tail()

heart_data.isnull().sum()

heart_data['chol'].unique()

heart_data['thalach'].unique()

heart_data.dtypes

"""Calculating the mean to replace the null values with it."""

mean = round(heart_data['chol'].mean(),2)
mean

heart_data["chol"].fillna(mean,inplace=True)
heart_data

mean_thalach = round(heart_data['thalach'].mean(),2)
mean_thalach

heart_data["thalach"].fillna(mean,inplace=True)
heart_data

"""Importing Data Visualization Libraries"""

import matplotlib.pyplot as plt
import seaborn as sns

!pip install hvplot

import hvplot.pandas

# Commented out IPython magic to ensure Python compatibility.
from scipy import stats
# %matplotlib inline
sns.set_style("whitegrid")
plt.style.use("fivethirtyeight")

heart_data.info()

heart_data.shape

heart_data.target.value_counts()

o, z = heart_data.target.value_counts()

data = {'Zeros':z, 'Ones':o}
targets = list(data.keys())
values = list(data.values())

plt.bar(targets, values)

have_disease = heart_data.loc[heart_data['target']==1, 'sex'].value_counts()
no_disease = heart_data.loc[heart_data['target']==0, 'sex'].value_counts()

import seaborn as sns

"""Finding out how many males and females have a heart disease"""

have_disease

no_disease

o, z = have_disease

data = {'Females':z, 'Males':o}
targets = list(data.keys())
values = list(data.values())

plt.bar(targets, values)
plt.title('Heart Disease by Gender')

categorical_val = []
continous_val = []
for column in heart_data.columns:
    if len(heart_data[column].unique()) <= 10:
        categorical_val.append(column)
    else:
        continous_val.append(column)

categorical_val

plt.figure(figsize=(15, 15))

for i, column in enumerate(continous_val, 1):
    plt.subplot(3, 2, i)
    heart_data[heart_data["target"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    heart_data[heart_data["target"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)

# Create another figure
plt.figure(figsize=(9, 7))

# Scatter with postivie examples
plt.scatter(heart_data.age[heart_data.target==1],
            heart_data.thalach[heart_data.target==1],
            c="salmon")

# Scatter with negative examples
plt.scatter(heart_data.age[heart_data.target==0],
            heart_data.thalach[heart_data.target==0],
            c="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"])

plt.figure(figsize=(15, 15))

for i, column in enumerate(categorical_val, 1):
    plt.subplot(3, 3, i)
    heart_data[heart_data["target"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    heart_data[heart_data["target"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)

# Let's make our correlation matrix a little prettier
corr_matrix = heart_data.corr()
fig, ax = plt.subplots(figsize=(15, 15))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)

dat = heart_data.drop('target', axis=1).corrwith(heart_data.target)

fig, ax = plt.subplots()

names = dat.keys()
vals = dat.values

y_pos = np.arange(len(names))

ax.barh(y_pos, vals, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(names)
ax.invert_yaxis()
ax.set_xlabel('Correlation')
ax.set_ylabel('Features')
ax.set_title('Correlation between Heart Disease and Numeric Features')

"""**Data Processing and Data Scaling**"""

scalable = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
for column in scalable:
  heart_data[column] = heart_data[column].apply(lambda x: (x - np.mean(heart_data[column]))/np.std(heart_data[column]))

heart_data.describe()

heart_data

"""Splitting the Features and Target"""

X = heart_data.drop(columns='target', axis=1)
Y = heart_data['target']

print(X)

"""Splitting the Data into Training data & Test Data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Model Training

Logistic Regression
"""

model = LogisticRegression()

# training the LogisticRegression model with Training data
model.fit(X_train, Y_train)

"""Model Evaluation"""

# accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data : ', training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy on Test data : ', test_data_accuracy)

"""Building a Predictive System"""

# medical parameters 
input_data = (62,0,0,140,268,0,0,160,0,3.6,0,2,2)

# change the input data to a numpy array
input_data_as_numpy_array= np.asarray(input_data)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]== 0):
  print('The Person does not have a Heart Disease')
else:
  print('The Person has Heart Disease')

"""**Models Building**"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")
        
    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))
        print("Test Result:\n================================================")        
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")

from sklearn.model_selection import train_test_split

X = heart_data.drop('target', axis=1)
y = heart_data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""1. Logistic Regression

"""

from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train, y_train)

print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)
print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100

results_df = pd.DataFrame(data=[["Logistic Regression", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df

"""2. K-nearest neighbors"""

from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)

print_score(knn_clf, X_train, y_train, X_test, y_test, train=True)
print_score(knn_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, knn_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, knn_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["K-nearest neighbors", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""3. Support Vector machine"""

from sklearn.svm import SVC


svm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)
svm_clf.fit(X_train, y_train)

print_score(svm_clf, X_train, y_train, X_test, y_test, train=True)
print_score(svm_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, svm_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, svm_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["Support Vector Machine", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""4. Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier


tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)

print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, tree_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, tree_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["Decision Tree Classifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""5. Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

test_score = accuracy_score(y_test, rf_clf.predict(X_test)) * 100
train_score = accuracy_score(y_train, rf_clf.predict(X_train)) * 100

results_df_2 = pd.DataFrame(data=[["Random Forest Classifier", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""1. Logistic Regression Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

params = {"C": np.logspace(-4, 4, 20),
          "solver": ["liblinear"]}

lr_clf = LogisticRegression()

lr_cv = GridSearchCV(lr_clf, params, scoring="accuracy", n_jobs=-1, verbose=1, cv=5)
lr_cv.fit(X_train, y_train)
best_params = lr_cv.best_params_
print(f"Best parameters: {best_params}")
lr_clf = LogisticRegression(**best_params)

lr_clf.fit(X_train, y_train)

print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)
print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)

accuracy_list = {'Classification Algorithm': ['LR', 'KNN', 'SVM', 'DTC','RFC'], 'Accuracy Level': [82.41, 76.92, 81.31,76.92,81.31]}  
  
a_df = pd.DataFrame(accuracy_list)
a_df

acc_fig = px.line(a_df, x="Classification Algorithm", y="Accuracy Level", title='Accuracy of various techniques',width=400, height=400)
acc_fig.update_traces(line=dict(color="Green", width=3))
acc_fig.show()

heart_data.groupby('cp').size().plot(kind='pie', autopct='%.2f')
plt.ylabel('Chest Pain')

heart_data.groupby('target').size().plot(kind='pie', autopct='%.2f')
plt.ylabel('Target')

LR0 = 0.804878
KNN0 = 0.738095
SVM0 = 0.815789
DTC0 = 0.717391
RF0 = 0.800000

LR1 = 0.84
KNN1 = 0.795918
SVM1 = 0.811321
DTC1 = 0.822222
RF1 = 0.823529

plt.figure(figsize = (12, 12))

plt.bar(np.arange(5) - 0.2, [LR0, KNN0, SVM0, DTC0, RF0], width = 0.5, label = '0')
plt.bar(np.arange(5) + 0.2, [LR1, KNN1, SVM1, DTC1, RF1], width = 0.5, label = '1')
plt.legend()
plt.xticks(np.arange(5), ['LR', 'KNN', 'SVM', 'DTC', 'RF'])
plt.xlabel('Techniques')
plt.ylabel('Precision%')

LR0 = 0.804878
KNN0 = 0.756098
SVM0 = 0.756098
DTC0 = 0.804878
RF0 = 0.780488

LR1 = 0.84
KNN1 = 0.780000
SVM1 = 0.860000
DTC1 = 0.740000
RF1 = 0.840000

plt.figure(figsize = (12, 12))

plt.bar(np.arange(5) - 0.2, [LR0, KNN0, SVM0, DTC0, RF0], width = 0.5, label = '0')
plt.bar(np.arange(5) + 0.2, [LR1, KNN1, SVM1, DTC1, RF1], width = 0.5, label = '1')
plt.legend()
plt.xticks(np.arange(5), ['LR', 'KNN', 'SVM', 'DTC', 'RF'])
plt.xlabel('Techniques')
plt.ylabel('Recall%')

LR0 = 0.804878
KNN0 = 0.746988
SVM0 = 0.784810
DTC0 = 0.758621
RF0 = 0.790123

LR1 = 0.84
KNN1 = 0.787879
SVM1 = 0.834951
DTC1 = 0.778947
RF1 = 0.831683

plt.figure(figsize = (12, 12))

plt.bar(np.arange(5) - 0.2, [LR0, KNN0, SVM0, DTC0, RF0], width = 0.5, label = '0')
plt.bar(np.arange(5) + 0.2, [LR1, KNN1, SVM1, DTC1, RF1], width = 0.5, label = '1')
plt.legend()
plt.xticks(np.arange(5), ['LR', 'KNN', 'SVM', 'DTC', 'RF'])
plt.xlabel('Techniques')
plt.ylabel('F1-Score%')

# libraries
import numpy as np
import matplotlib.pyplot as plt
 
# create a dataset
height = [82.41, 76.92, 81.31, 76.92, 81.31]
bars = ('LR', 'KNN', 'SVM', 'DTC', 'RF')
x_pos = np.arange(len(bars))

# Create bars with different colors
plt.bar(x_pos, height, color=['black', 'red', 'green', 'blue', 'cyan'])

# Create names on the x-axis
plt.xticks(x_pos, bars)

plt.xlabel('Techniques')
plt.ylabel('Accuracy')


# Show graph
plt.show()